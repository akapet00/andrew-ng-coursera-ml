{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Learning\n",
    "\n",
    "## 1 Neural Network\n",
    "\n",
    "Trough this excercise, the backpropagation algorithm for neural networks is developed and implemented to learn hand-written digit recognition problem.\n",
    "\n",
    "\n",
    "### 1.1 Data visualization\n",
    "\n",
    "There are 5000 training examples in *ex4data1.mat*, where each training example is a 20 pixel by 20 pixel grayscale image of the digit. Each pixel is represented by a floating point number indicating the grayscale intensity at that location. The 20 by 20 grid of pixels is “unrolled” into a 400-dimensional vector. Each of these training examples becomes a single row in our data matrix X. This results in a 5000 by 400 matrix **X** where every row is a training example for a handwritten digit image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "from scipy.optimize import minimize, fmin_cg\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011', '__version__': '1.0', '__globals__': [], 'X': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), 'y': array([[10],\n",
      "       [10],\n",
      "       [10],\n",
      "       ...,\n",
      "       [ 9],\n",
      "       [ 9],\n",
      "       [ 9]], dtype=uint8)}\n",
      "X matrix shape: (5000, 400)\n",
      "y vector shape: (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset, load and visualization\n",
    "dataset = sio.loadmat('ex4data1.mat')\n",
    "print(dataset)\n",
    "X = dataset['X']\n",
    "y = dataset['y']\n",
    "y[:500] = 0\n",
    "print(f'X matrix shape: {X.shape}')\n",
    "print(f'y vector shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rand_indices = np.random.randint(low=0, high=X.shape[0], size=4)\n",
    "plt.figure(figsize=(7,7))\n",
    "for i, rand_index in enumerate(rand_indices):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.title(f'label y = {y[rand_index]}')\n",
    "    plt.gray()\n",
    "    plt.grid()\n",
    "    plt.imshow(X[rand_index].reshape(20,20).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Model representation\n",
    "\n",
    "To start with the feedforward algorithm, parameters theta 1 and theta 2 (weights between input and hidden layer, and hidden and output layer) are stored in *ex4weights.mat*.\n",
    "![nn](nn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Tue Oct 18 14:57:02 2011'\n",
      "Theta1 shape: (25, 401)\n",
      "Theta2 shape: (10, 26)\n"
     ]
    }
   ],
   "source": [
    "# initial weights\n",
    "weights = sio.loadmat('ex4weights.mat')\n",
    "print(weights['__header__'])\n",
    "Theta1 = weights['Theta1']\n",
    "Theta2 = weights['Theta2']\n",
    "print(f'Theta1 shape: {Theta1.shape}')\n",
    "print(f'Theta2 shape: {Theta2.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Feedforward and cost funtion\n",
    "\n",
    "A neural network's cost function (without regularization) is defined as:\n",
    "$$ J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}[-y^{(i)}\\log(h_{\\theta}(x^{(i)})) - (1-y^{(i)})\\log(1-h_{\\theta}(x^{i}))]$$\n",
    "\n",
    "where $h_{\\theta}(x^{(i)})$ is computed as shown in Model representation figure and $ K $} is the total number of possible labels, in this case $ K = 10 $. \n",
    "\n",
    "In this case, where the neural network consists of 3 layer, term $h_{\\theta}(x^{(i)})$ is the activation (output value) of the *k*-th output unit, $a_{k}^{(3)}$, where the output unit is calculated via feedforward algorithm as follows:\n",
    "$$\n",
    "z^{(2)} = \\theta^{(1)} x\n",
    "$$\n",
    "\n",
    "where $x$ can be written as $x=a^{(1)}$,\n",
    "$$\n",
    "a^{(2)} = g(z^{(2)}) \n",
    "$$\n",
    "\n",
    "$$\n",
    "a_{0}^{2} = +1\n",
    "$$\n",
    "\n",
    "Finally, we have value for $z^{(3)}$:\n",
    "$$\n",
    "z^{(3)} = \\theta^{(2)} a^{(2)}\n",
    "$$\n",
    "\n",
    "Output is then defined as:\n",
    "$$\n",
    "h_{\\theta}(x) = a^{(3)} = g(z^{(3)})\n",
    "$$\n",
    "where $ g $ is sigmoid function defined as:\n",
    "\n",
    "$$ \n",
    "g(z) = \\frac{1}{1 + e^{-z}}.\n",
    "$$\n",
    "\n",
    "Also, note that y has to be encoded in a way that labels 0, 1, ..., 9 are vectors containing only values 0 or 1.\n",
    "\n",
    "After the regularization is applied, cost function is defined as:\n",
    "$$ J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}[-y^{(i)}\\log(h_{\\theta}(x^{(i)})) - (1-y^{(i)})\\log(1-h_{\\theta}(x^{i}))] + + \\frac{\\lambda}{2m}[\\sum_{j=1}^{25}\\sum_{k=1}^{400}(\\theta_{j,k}^{(1)})^2 + \\sum_{j=1}^{10}\\sum_{k=1}^{25}(\\theta_{j,k}^{(2)})^2] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture\n",
    "input_layer_size = 400\n",
    "hidden_layer_size = 25\n",
    "num_labels = 10\n",
    "nn_params = np.r_[Theta1.ravel(), Theta2.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def labelEncoder(y):\n",
    "    # encode y to (5000, 10) sparse matrix\n",
    "    enc = OneHotEncoder(categories='auto')\n",
    "    return enc.fit_transform(y).toarray()\n",
    "\n",
    "def feedforward(X, Theta1, Theta2):\n",
    "    # number of training examples\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # input layer\n",
    "    X_bias = np.concatenate((np.ones((m, 1)), X), axis=1)\n",
    "    # hidden layer\n",
    "    z2 = X_bias @ Theta1.T # (5000, 25) = (5000, 401) x (401, 25)\n",
    "    a2 = sigmoid(z2) # (5000, 25)\n",
    "    a2 = np.concatenate((np.ones((m, 1)), a2), axis=1) # (5000, 26)\n",
    "    # output layer - hypothesis\n",
    "    z3 = a2 @ Theta2.T # (5000, 10) = (5000, 26) x (26,10)\n",
    "    a3 = sigmoid(z3) # (5000, 10)\n",
    "    return z2, a2, z3, a3\n",
    "   \n",
    "def nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, _lambda=0):\n",
    "    # recovering weights from flattened array\n",
    "    Theta1 = nn_params[:((input_layer_size + 1) * hidden_layer_size)].reshape(hidden_layer_size, input_layer_size+1)\n",
    "    Theta2 = nn_params[((input_layer_size + 1) * hidden_layer_size):].reshape(num_labels, (hidden_layer_size+1))\n",
    "    \n",
    "    # number of training examples\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # encoded y\n",
    "    y_enc = labelEncoder(y)\n",
    "    \n",
    "    # single forward pass\n",
    "    z2, a2, z3, a3 = feedforward(X, Theta1, Theta2)\n",
    "    \n",
    "    # cost function with regularization term (not applied on first column of bias term of theta 1 and 2)\n",
    "    err = (_lambda/(2*m)) * (np.sum(np.sum(Theta1[:,1:]**2) + np.sum(np.sum(Theta2[:,1:]**2))))\n",
    "    J = -1*(1/m) *np.sum((np.log(a3)*(y_enc) + np.log(1-a3)*(1-y_enc)))\n",
    "    J = J + err\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of the initial cost (_lambda = 0): 0.2876291651613189\n",
      "Value of the initial cost with pretrained weights should be cca 0.287629\n",
      "Value of the initial cost (_lambda = 1): 0.38376985909092365\n",
      "Value of the initial cost with pretrained weights should be cca 0.383769\n",
      "\n",
      "Bad fit for my situation :)\n",
      "Value of the initial cost (_lambda = 0): 10.44145967277798\n",
      "Value of the initial cost (_lambda = 1): 10.537600366707585\n"
     ]
    }
   ],
   "source": [
    "# in this example we use already trained network's parameters theta1 and theta2\n",
    "# stored into nn_params\n",
    "# cost function is large because i changed the dataset a little bit, instead of \n",
    "# labeling 0s as 10, I label 0s as 0s\n",
    "# to prove that, here, we perform calculation of cost both with engineered and \n",
    "# initial y values\n",
    "\n",
    "# original y\n",
    "y_orig = np.copy(y)\n",
    "y_orig[:500] = 10 # label 0s as 10s\n",
    "# cost without regularization \n",
    "_lambda = 0 \n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y_orig, _lambda)\n",
    "print(f'Value of the initial cost (_lambda = {_lambda}): {J}')\n",
    "print('Value of the initial cost with pretrained weights should be cca 0.287629')\n",
    "\n",
    "# cost without regularization\n",
    "_lambda = 1\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y_orig, _lambda)\n",
    "print(f'Value of the initial cost (_lambda = {_lambda}): {J}')\n",
    "print('Value of the initial cost with pretrained weights should be cca 0.383769')\n",
    "\n",
    "print()\n",
    "print('Bad fit for my situation :)')\n",
    "# my y, which is actually how it suppose to be, without the silly matlab indexing\n",
    "# cost without regularization \n",
    "_lambda = 0 \n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, _lambda)\n",
    "print(f'Value of the initial cost (_lambda = {_lambda}): {J}')\n",
    "\n",
    "# cost without regularization\n",
    "_lambda = 1\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, _lambda)\n",
    "print(f'Value of the initial cost (_lambda = {_lambda}): {J}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Backpropagation\n",
    "\n",
    "The backpropagation algorithm cmputes the gradient for the neural network cost function. \n",
    "\n",
    "Once the gradient is computed, the neural network will be ready for training (minimizing the cost function using some form of optimization function such as *fmincg*).\n",
    "\n",
    "### 2.1 Sigmoid gradient\n",
    "\n",
    "The gradient for the sigmoid function can be computed as:\n",
    "$$\n",
    "g'(z) = \\frac{d}{dz}g(z) = g(z)(1-g(z))\n",
    "$$\n",
    "\n",
    "where $g(z)$ is sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Random initialization\n",
    "\n",
    "When training neural networks, it is important to randomly initialize the parameters for symmetry breaking. One effective strategy for random initialization is to randomly select values for weights uniformly in the range $[-\\epsilon_{init}, \\epsilon_{init}]$.\n",
    "\n",
    "The effective strategy for choosing $\\epsilon_{init}$ is to base it on the number of units in the network, for example:\n",
    "$$\n",
    "\\epsilon_{init} = \\frac{\\sqrt{6}}{\\sqrt{L_{in} + L_{out}}}\n",
    "$$\n",
    "\n",
    "where $L_{in}$ is the number of input units and $L_{out}$ is the number of inputs in the adjacent layer plus 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out):\n",
    "    epsilon_init = 0.12\n",
    "    return (np.random.uniform(low=0.0, high=1.0, \n",
    "                             size=(L_out, 1 + L_in)) \\\n",
    "            * 2 * epsilon_init) - epsilon_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "\n",
    "initial_nn_params = np.r_[initial_Theta1.ravel(), initial_Theta2.ravel()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Backpropagation\n",
    "\n",
    "For each training example $(x^{(i)}, y^{(i)})$, we first performe a forward pass to compute all activations throughout the network, including the output hypothesis and then, for each node $j$ in layel $l$, we compute error term $\\delta_{j}^{(l)}$.\n",
    "All gradients should be accumulated in the capital delta matrix.\n",
    "\n",
    "![nn](backprop.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, _lambda=0):\n",
    "    m = X.shape[0] # 5000\n",
    "    X_bias = np.concatenate((np.ones((m, 1)), X), axis=1) # (5000, 401)\n",
    "    y = labelEncoder(y) # (5000, 10)\n",
    "    \n",
    "    Theta1 = nn_params[:((input_layer_size + 1) * hidden_layer_size)].reshape(hidden_layer_size, input_layer_size+1) # (25, 401)\n",
    "    Theta2 = nn_params[((input_layer_size + 1) * hidden_layer_size):].reshape(num_labels, (hidden_layer_size+1)) # (10, 26)\n",
    "    z2, a2, z3, a3 = feedforward(X, Theta1, Theta2)\n",
    "    z2_bias = np.concatenate((np.ones((z2.shape[0], 1)), z2), axis=1)\n",
    "    \n",
    "    delta1 = np.zeros(shape=(Theta1.shape)) # (25, 401)\n",
    "    delta2 = np.zeros(shape=(Theta2.shape)) # (10, 26)\n",
    "    \n",
    "    for t in range(m): # for every example, fordward & back prop\n",
    "        a1t = X_bias[t,:].reshape(1, -1) # (1, 401)\n",
    "        a2t = a2[t,:].reshape(1, -1) # (1, 26)\n",
    "        a3t = a3[t,:].reshape(1, -1) # (1, 10)\n",
    "        yt = y[t,:].reshape(1, -1) # (1, 10)\n",
    "        z2t = z2_bias[t, :].reshape(1, -1) # (1, 26)\n",
    "        \n",
    "        d3 = a3t - yt # (1, 10) = (1, 10) - (1, 10)\n",
    "        d2 = Theta2.T @ d3.T * sigmoidGradient(z2t.T) # (26, 1) = (26, 10) x (10 ,1) * (26, 1)\n",
    "        delta1 = delta1 + d2[1:, :] @ a1t # (25, 401) = (25, 401) + (25, 1) x (1, 401)\n",
    "        delta2 = delta2 + d3.T @ a2t # (10, 26) = (10, 26) + (10, 1) x (1, 26)\n",
    "    regTheta1 = Theta1[:,1:]\n",
    "    regTheta2 = Theta2[:,1:]\n",
    "    Theta1_grad = (1/m) * delta1 + (_lambda/m) * np.concatenate((np.zeros((Theta1.shape[0],1)), regTheta1), axis=1)\n",
    "    Theta2_grad = (1/m) * delta2 + (_lambda/m) * np.concatenate((np.zeros((Theta2.shape[0],1)), regTheta2), axis=1)\n",
    "    \n",
    "    return np.concatenate((Theta1_grad.ravel(), Theta2_grad.ravel()))\n",
    "    \n",
    "    \n",
    "def vecBackprop(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, _lambda=0):\n",
    "    m = X.shape[0]\n",
    "    X_bias = np.concatenate((np.ones((m, 1)), X), axis=1)\n",
    "    y_enc = labelEncoder(y)\n",
    "    \n",
    "    Theta1 = nn_params[:((input_layer_size + 1) * hidden_layer_size)].reshape(hidden_layer_size, input_layer_size+1)\n",
    "    Theta2 = nn_params[((input_layer_size + 1) * hidden_layer_size):].reshape(num_labels, (hidden_layer_size+1))\n",
    "    z2, a2, z3, a3 = feedforward(X, Theta1, Theta2)\n",
    "    \n",
    "    d3 = a3 - y_enc # (5000, 10)\n",
    "    d2 = Theta2[:,1:].T @ d3.T * sigmoidGradient(z2.T) # (25, 5000) = (25, 10) x (10, 5000) * (25, 5000)\n",
    "    \n",
    "    delta1 = d2 @ X_bias # (25, 401) = (25, 5000) x (5000, 401)\n",
    "    delta2 = d3.T @ a2 # (10, 26) = (10, 5000) x (5000, 26)\n",
    "    \n",
    "    Theta1_ = np.c_[np.ones((Theta1.shape[0],1)), Theta1[:,1:]]\n",
    "    Theta2_ = np.c_[np.ones((Theta2.shape[0],1)), Theta2[:,1:]]\n",
    "    \n",
    "    Theta1_grad = (delta1/m) + (Theta1_*_lambda)/m\n",
    "    Theta2_grad = (delta2/m) + (Theta2_*_lambda)/m\n",
    "\n",
    "    return np.concatenate([Theta1_grad.ravel(), Theta2_grad.ravel()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizeTheta(initial_nn_params, X, y, _lambda):\n",
    "    fmin = minimize(nnCostFunction, \n",
    "                    initial_nn_params, \n",
    "                    args=(input_layer_size, \n",
    "                          hidden_layer_size, \n",
    "                          num_labels,\n",
    "                          X, \n",
    "                          y, \n",
    "                          _lambda),\n",
    "                    jac=vecBackprop, \n",
    "                    method='TNC',\n",
    "                    options={'maxiter': 100,\n",
    "                             'disp':True})\n",
    "    opt = fmin.x \n",
    "    return opt, fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt, opt_history = optimizeTheta(initial_nn_params, X, y, _lambda)\n",
    "Theta1 = opt[:((input_layer_size + 1) * hidden_layer_size)].reshape(hidden_layer_size, input_layer_size+1)\n",
    "Theta2 = opt[((input_layer_size + 1) * hidden_layer_size):].reshape(num_labels, (hidden_layer_size+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 9, 9, 9])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_full = feedforward(X, Theta1, Theta2)\n",
    "pred = np.argmax(ff_full[-1], axis=1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set accuracy: 97.18\n"
     ]
    }
   ],
   "source": [
    "print(f'training set accuracy: {(np.mean(pred == y.flatten()) * 100)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'theta_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-c49cb8217722>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mJ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcostFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_norm_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'# of iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'theta_history' is not defined"
     ]
    }
   ],
   "source": [
    "J = []\n",
    "for i, th in enumerate(theta_history):\n",
    "    J.append(costFunc(th, X_norm_bias, y))  \n",
    "plt.plot(J)\n",
    "plt.xlabel('# of iteration')\n",
    "plt.ylabel('cost function value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.5661711439380165\n",
       "     jac: array([ 3.80499432e-03,  6.82852844e-06,  1.93670437e-05, ...,\n",
       "       -5.83654209e-04, -1.34977518e-03, -9.79732252e-04])\n",
       " message: 'Max. number of function evaluations reached'\n",
       "    nfev: 100\n",
       "     nit: 11\n",
       "  status: 3\n",
       " success: False\n",
       "       x: array([-0.62053511,  0.03414264,  0.09683522, ...,  2.8911043 ,\n",
       "       -0.66186032, -2.92618429])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
